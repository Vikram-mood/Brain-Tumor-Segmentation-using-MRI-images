
Title: Brain Tumor Segmentation in MRI images.
Objective
The primary objective of this project is to develop and train deep learning models
for accurate and automated segmentation of brain tumors in multimodal
Magnetic Resonance Imaging (MRI) scans from the BraTS 2020 dataset.
Specifically, the goal is to:
1. Segment Tumor Sub-regions: Accurately delineate three tumor sub-regions
2. 3. 4. in MRI scans:
○
Necrotic and non-enhancing tumor core (NCR/NET, Label 1).
○
Peritumoral edema (ED, Label 2).
○
GD-enhancing tumor (ET, Label 3).
○
Exclude non-tumor regions (background, Label 0).
Leverage Multimodal MRI Data: Utilize the most informative MRI
modalities (T1ce and T2-FLAIR) to capture complementary information
about brain anatomy and tumor characteristics, improving segmentation
performance.
Optimize Model Performance: Implement and compare advanced
convolutional neural network (CNN) architectures (U-Net, U-Net++,
ResNet+U-Net, Attention ResNet+U-Net, and Attention with U-Net) to
achieve high segmentation accuracy, particularly for small and clinically
significant tumor regions.
Evaluate Robustness: Use a categorical cross-entropy loss and evaluation
metrics (accuracy, IoU, Dice coefficient, sensitivity, precision, specificity,
and class-specific Dice coefficients) to comprehensively assess model
performance.
Overview
The Brain Tumor Segmentation project leverages the BraTS 2020 dataset to train
deep learning models for segmenting brain tumors in multimodal MRI scans. The
project involves preprocessing the data, selecting optimal modalities, designing
and training CNN-based models, and evaluating performance with tailored metrics.
Below is a detailed overview of the key components:
1. Dataset
●
BraTS 2020 Dataset: A collection of 369 patients multimodal MRI scans
from glioma patients, including four modalities per patient:
○
○
T1 (Native): Highlights tissue structure.
T1ce (Post-contrast T1-weighted): Enhances abnormalities with
Gadolinium contrast.
○
○
○
T2 (T2-weighted): Emphasizes fluid content.
T2-FLAIR: Suppresses fluid signals to highlight lesions, especially in
white matter.
Each modality has 155 slices.
●
Selected Modalities:
○
T1ce and T2-FLAIR are used for training, as they provide clearer and
complementary information about tumor regions.
○
T1 and T2 are excluded to avoid redundancy and reduce noise from
fluid signals that could hinder predictions.
Annotations: Expert-annotated segmentation masks with four labels:
●
●
●
●
Label 0: Not Tumor (background, overrepresented).
Label 1: Necrotic and non-enhancing tumor core (NCR/NET).
Label 2: Peritumoral edema (ED).
Label 4: GD-enhancing tumor .
Data Structure:
●
Each sample is a 3D volume (width, height, depth) composed of 2D slices
along three planes: axial, coronal, and sagittal.
●
Only informative slices are used to reduce training time and focus on
tumor-containing regions.
2. Preprocessing
●
Median Filter: Applied to reduce noise in MRI images while preserving
●
●
edges.
Bias Field Correction: Corrects intensity non-uniformities caused by
magnetic field variations, improving image consistency.
Resampling: Standardizes the spatial resolution across images for uniform
input to the model.
Note: We preprocessed the data, but we got noisy images as output. so we
continued the training with raw data. Here, the input image is a preprocessed
image.
3. Models
The project evaluates multiple CNN architectures for segmentation, each designed
to capture spatial and contextual features in MRI images:
●
●
●
●
●
U-Net: A standard encoder-decoder architecture with skip connections,
effective for medical image segmentation.
U-Net++: An enhanced U-Net with nested skip connections to improve
feature propagation and segmentation accuracy.
ResNet+U-Net: Combines a ResNet backbone (for robust feature extraction)
with a U-Net decoder, leveraging pretrained weights.
Attention ResNet+U-Net: Integrates attention gates with ResNet+U-Net to
focus on relevant regions, improving performance on small tumor areas.
Attention with U-Net: Adds attention mechanisms to the standard U-Net to
enhance focus on tumor sub-regions.
4. Loss Functions
●
●
Categorical Cross-Entropy: Measures the difference between predicted
probability distributions and one-hot encoded ground truth labels for each
pixel, suitable for multi-class segmentation.
Dice Loss: Focuses on the overlap between predicted and ground truth
segmentations, addressing class imbalance by prioritizing small tumor
regions.
5. Evaluation Metrics
A comprehensive set of metrics is used to evaluate model performance,
particularly addressing the class imbalance in BraTS:
●
●
●
●
●
●
●
Accuracy: Proportion of correctly classified pixels. Less reliable due to
background class dominance.
Intersection over Union (IoU): Measures overlap between predicted and
ground truth segmentations, averaged across classes.
Dice Coefficient: Quantifies similarity between predicted and ground truth
regions, sensitive to small tumor areas.
Sensitivity (Recall): Proportion of true positive pixels correctly identified,
critical for detecting tumor regions.
Precision: Proportion of predicted positive pixels that are correct, indicating
prediction reliability.
Specificity: Proportion of true negative pixels correctly identified, important
for avoiding false positives.
Class-Specific Dice Coefficients:
○
dice
coef
_
_
necrotic: Dice score for the necrotic core (Label 1).
○
dice
coef
_
_
edema: Dice score for peritumoral edema (Label 2).
○
dice
coef
_
_
enhancing: Dice score for the enhancing tumor (Label 3).
Training Configuration:
Parameters Values
Epochs 10
Optimizer Adam
Optimizer
Loss Categorical
Cross Entropy
Learning
rate
0.001
Model: Unet
Results:
Model: Unet++
Model: Resnet+Unet
Model: Attention based Unet
Model: Attention based Resnet+Unet
Model Loss Accuracy MeanIOU Dice
coefficient
Precision Sensitivity Specificity Dice coef
Necrotic
Dice coef
Edema
Dice coef
Enhancing
Unet 0.0282 0.9903 0.7713 0.4441 0.9922 0.988 0.9973 0.2884 0.6169 0.383 Unet++ Resnet+Unet Attention
based Unet
Attention
based
Resnet+Unet
0.0301 0.2123 0.0641 0.1422
0.9907 0.9478 0.985 0.9622
0.8424 0.3756 0.7667 0.3756
0.4674 0.412 0.2801 0.4173
0.9915 0.9518 0.9849 0.9657
0.9895 0.9459 0.9848 0.9595
0.9971 0.9842 0.995 0.9887
0.2672 0.3267 0.0712 0.3588
0.5453 0.2569 0.1644 0.325
0.4757 0.4147 0.0655 0.4043
Conclusion
Among the evaluated models, U-Net++ emerges as the most effective for brain
tumor segmentation on raw T1ce and T2-FLAIR MRI scans from the BraTS
2020 dataset, achieving the highest Mean IoU (0.8424), Dice coefficient
(0.4674), and enhancing tumor Dice (0.4757), with competitive performance
across other metrics. Its nested skip connections and robustness to raw data
variability make it ideal for clinical applications, where accurate segmentation
of enhancing tumors is critical. However, all models face challenges with class
imbalance (background dominance) and small tumor regions (low necrotic and
enhancing Dice scores), exacerbated by raw data noise and variability. U-Net is
a strong alternative with the lowest loss and high edema Dice, while
Attention-based ResNet+U-Net excels for necrotic regions but underperforms
overall. To improve performance, future work should focus on revisiting
preprocessing to address raw data challenges. These improvements could
further enhance segmentation accuracy.
